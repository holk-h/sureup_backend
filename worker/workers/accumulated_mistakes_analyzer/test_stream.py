"""
æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½

ç”¨äºéªŒè¯ç«å±±å¼•æ“ LLM æµå¼ API æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import os
import sys
import asyncio
from pathlib import Path

# æ·»åŠ  mistake_analyzer è·¯å¾„ä»¥å¯¼å…¥ llm_provider
mistake_analyzer_path = Path(__file__).parent.parent / 'mistake_analyzer'
sys.path.insert(0, str(mistake_analyzer_path))

from llm_provider import get_llm_provider


async def test_stream_output():
    """æµ‹è¯•æµå¼è¾“å‡º"""
    print("=" * 60)
    print("æµ‹è¯•ç«å±±å¼•æ“æµå¼è¾“å‡º")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ– LLM Provider
    provider = get_llm_provider()
    
    # ç®€å•çš„æµ‹è¯• prompt
    prompt = """è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯é”™é¢˜æœ¬ï¼Œä»¥åŠå¦‚ä½•æœ‰æ•ˆä½¿ç”¨é”™é¢˜æœ¬ã€‚
    
è¦æ±‚ï¼š
- åˆ† 3-4 æ®µå›ç­”
- æ¯æ®µ 2-3 å¥è¯
- ä½¿ç”¨ Markdown æ ¼å¼
- åŒ…å« emoji
"""
    
    print("å¼€å§‹æµå¼ç”Ÿæˆ...\n")
    print("-" * 60)
    
    # è°ƒç”¨æµå¼ API
    stream_response = await provider.chat(
        prompt=prompt,
        temperature=0.7,
        max_tokens=1000,
        stream=True
    )
    
    # å¤„ç†æµå¼å“åº”
    accumulated_content = ''
    chunk_count = 0
    
    try:
        with stream_response:
            for chunk in stream_response:
                chunk_count += 1
                
                # æå–å¢é‡å†…å®¹
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content is not None:
                        accumulated_content += delta.content
                        # å®æ—¶æ‰“å°ï¼ˆä¸æ¢è¡Œï¼‰
                        print(delta.content, end='', flush=True)
        
        print("\n")
        print("-" * 60)
        print(f"\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼")
        print(f"   - æ¥æ”¶åˆ° {chunk_count} ä¸ª chunk")
        print(f"   - æ€»å†…å®¹é•¿åº¦: {len(accumulated_content)} å­—ç¬¦")
        print()
        
        return True
        
    except Exception as e:
        print(f"\n\nâŒ æµå¼è¾“å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_update_frequency():
    """æµ‹è¯• 0.5 ç§’æ›´æ–°é¢‘ç‡"""
    print("=" * 60)
    print("æµ‹è¯• 0.5 ç§’æ›´æ–°é¢‘ç‡")
    print("=" * 60)
    print()
    
    provider = get_llm_provider()
    
    prompt = """è¯·å†™ä¸€ç¯‡ 200 å­—å·¦å³çš„çŸ­æ–‡ï¼Œä¸»é¢˜æ˜¯ï¼šå­¦ä¹ çš„æ„ä¹‰ã€‚

è¦æ±‚ï¼š
- åˆ†æ®µæè¿°
- ä½¿ç”¨ Markdown æ ¼å¼
"""
    
    print("å¼€å§‹æµå¼ç”Ÿæˆï¼ˆæ¨¡æ‹Ÿæ•°æ®åº“æ›´æ–°ï¼‰...\n")
    print("-" * 60)
    
    stream_response = await provider.chat(
        prompt=prompt,
        temperature=0.7,
        max_tokens=500,
        stream=True
    )
    
    accumulated_content = ''
    last_update_time = asyncio.get_event_loop().time()
    update_interval = 0.5  # 0.5 ç§’æ›´æ–°ä¸€æ¬¡
    update_count = 0
    
    try:
        with stream_response:
            for chunk in stream_response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content is not None:
                        accumulated_content += delta.content
                        print(delta.content, end='', flush=True)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦"æ›´æ–°æ•°æ®åº“"
                        current_time = asyncio.get_event_loop().time()
                        if current_time - last_update_time >= update_interval:
                            update_count += 1
                            print(f"\n[ğŸ“ æ¨¡æ‹Ÿæ•°æ®åº“æ›´æ–° #{update_count}ï¼Œå†…å®¹é•¿åº¦: {len(accumulated_content)}]", end='')
                            last_update_time = current_time
        
        # æœ€åä¸€æ¬¡æ›´æ–°
        update_count += 1
        print(f"\n[ğŸ“ æ¨¡æ‹Ÿæ•°æ®åº“æ›´æ–° #{update_count}ï¼ˆæœ€ç»ˆï¼‰ï¼Œå†…å®¹é•¿åº¦: {len(accumulated_content)}]")
        
        print("\n")
        print("-" * 60)
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"   - æ€»å…± {update_count} æ¬¡æ•°æ®åº“æ›´æ–°")
        print(f"   - å¹³å‡æ›´æ–°é—´éš”: {update_interval} ç§’")
        print(f"   - æœ€ç»ˆå†…å®¹é•¿åº¦: {len(accumulated_content)} å­—ç¬¦")
        print()
        
        return True
        
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print()
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½")
    print()
    
    # æµ‹è¯• 1: åŸºæœ¬æµå¼è¾“å‡º
    result1 = await test_stream_output()
    
    if result1:
        # æµ‹è¯• 2: æ›´æ–°é¢‘ç‡
        await asyncio.sleep(2)  # é—´éš”ä¸€ä¸‹
        result2 = await test_update_frequency()
        
        if result2:
            print()
            print("=" * 60)
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            print("=" * 60)
            print()
            print("è¯´æ˜ï¼š")
            print("  - æµå¼è¾“å‡ºåŠŸèƒ½æ­£å¸¸")
            print("  - 0.5 ç§’æ›´æ–°é¢‘ç‡åˆç†")
            print("  - å¯ä»¥åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒ")
            print()
        else:
            print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    else:
        print("\nâš ï¸ åŸºæœ¬æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡åç»­æµ‹è¯•")


if __name__ == '__main__':
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ['DOUBAO_API_KEY', 'DOUBAO_MODEL']
    missing_vars = [var for var in required_vars if not os.environ.get(var)]
    
    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print()
        print("è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š")
        print("  export DOUBAO_API_KEY='your_api_key'")
        print("  export DOUBAO_MODEL='your_model_endpoint_id'")
        print()
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())

