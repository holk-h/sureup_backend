"""
ç§¯ç´¯é”™é¢˜åˆ†æ Worker å®ç°

è´Ÿè´£åˆ†æç”¨æˆ·ç§¯ç´¯çš„é”™é¢˜ï¼Œç”Ÿæˆå­¦ä¹ å»ºè®®
æ”¯æŒé€šè¿‡ Realtime API è¿›è¡Œæµå¼è¾“å‡º
"""
import os
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from loguru import logger

from workers.base import BaseWorker

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
def get_databases():
    from appwrite.client import Client
    from appwrite.services.databases import Databases
    client = Client()
    client.set_endpoint(os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1'))
    client.set_project(os.environ['APPWRITE_PROJECT_ID'])
    client.set_key(os.environ['APPWRITE_API_KEY'])
    return Databases(client)


def get_llm_provider():
    """è·å– LLM Providerï¼ˆå¤ç”¨ mistake_analyzer çš„ä»£ç ï¼‰"""
    import sys
    from pathlib import Path
    
    # æ·»åŠ  mistake_analyzer è·¯å¾„
    mistake_analyzer_path = Path(__file__).parent.parent / 'mistake_analyzer'
    sys.path.insert(0, str(mistake_analyzer_path))
    
    from llm_provider import get_llm_provider as _get_llm_provider
    return _get_llm_provider()


DATABASE_ID = os.environ.get('APPWRITE_DATABASE_ID', 'main')
COLLECTION_ANALYSES = 'accumulated_analyses'
COLLECTION_MISTAKES = 'mistake_records'
COLLECTION_QUESTIONS = 'questions'
COLLECTION_USER_KP = 'user_knowledge_points'


# å­¦ç§‘ä¸­æ–‡æ˜ å°„
SUBJECT_NAMES = {
    'math': 'æ•°å­¦',
    'physics': 'ç‰©ç†',
    'chemistry': 'åŒ–å­¦',
    'biology': 'ç”Ÿç‰©',
    'chinese': 'è¯­æ–‡',
    'english': 'è‹±è¯­',
    'history': 'å†å²',
    'geography': 'åœ°ç†',
    'politics': 'æ”¿æ²»'
}

# é”™å› ä¸­æ–‡æ˜ å°„
ERROR_REASON_NAMES = {
    'conceptUnclear': 'æ¦‚å¿µç†è§£ä¸æ¸…',
    'logicBlocked': 'æ€è·¯æ–­äº†',
    'calculationError': 'è®¡ç®—é”™è¯¯',
    'careless': 'ç²—å¿ƒå¤§æ„',
    'unfamiliar': 'çŸ¥è¯†ç›²åŒº',
    'timeInsufficient': 'æ—¶é—´ä¸å¤Ÿ',
    'other': 'å…¶ä»–'
}


class AccumulatedMistakesAnalyzerWorker(BaseWorker):
    """ç§¯ç´¯é”™é¢˜åˆ†æ Worker"""
    
    def __init__(self):
        super().__init__()
        self.databases = None
        self.llm_provider = None
    
    def _init_services(self):
        """åˆå§‹åŒ–æœåŠ¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not self.databases:
            self.databases = get_databases()
        if not self.llm_provider:
            self.llm_provider = get_llm_provider()
    
    async def process(self, task_data: Dict[str, Any]) -> Any:
        """
        å¤„ç†ç§¯ç´¯é”™é¢˜åˆ†æä»»åŠ¡
        
        Args:
            task_data: {
                'analysis_id': 'åˆ†æè®°å½•ID',
                'user_id': 'ç”¨æˆ·ID',
                'mistake_count': 15,
                'days_since_last_review': 3
            }
        
        Returns:
            åˆ†æç»“æœ
        """
        analysis_id = task_data.get('analysis_id')
        user_id = task_data.get('user_id')
        
        if not analysis_id or not user_id:
            raise ValueError("ç¼ºå°‘ analysis_id æˆ– user_id")
        
        logger.info(f"å¼€å§‹åˆ†æç”¨æˆ· {user_id} çš„ç§¯ç´¯é”™é¢˜ï¼Œåˆ†æID: {analysis_id}")
        
        # åˆå§‹åŒ–æœåŠ¡
        self._init_services()
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸º processing
            await self._update_analysis_status(analysis_id, 'processing')
            
            # 1. è·å–ç”¨æˆ·ç§¯ç´¯çš„é”™é¢˜
            mistakes = await self._get_accumulated_mistakes(user_id, task_data)
            
            if not mistakes:
                logger.info(f"ç”¨æˆ· {user_id} æ²¡æœ‰ç§¯ç´¯é”™é¢˜")
                await self._update_analysis_status(
                    analysis_id, 
                    'completed',
                    content='æš‚æ—¶è¿˜æ²¡æœ‰ç§¯ç´¯é”™é¢˜å“¦ï¼Œè®°å½•é”™é¢˜åå†æ¥åˆ†æå§ï¼'
                )
                return {'success': True, 'message': 'æ²¡æœ‰ç§¯ç´¯é”™é¢˜'}
            
            # 2. ç»Ÿè®¡åˆ†ææ•°æ®
            stats = await self._calculate_statistics(mistakes, user_id)
            
            # 3. ç”Ÿæˆåˆ†æå†…å®¹ï¼ˆæµå¼è¾“å‡ºï¼‰
            await self._generate_analysis(analysis_id, mistakes, stats)
            
            # 4. æ ‡è®°æ‰€æœ‰é”™é¢˜ä¸ºå·²åˆ†æ
            await self._mark_mistakes_as_analyzed(mistakes)
            
            # 5. æ›´æ–°ä¸ºå®ŒæˆçŠ¶æ€
            await self._update_analysis_status(
                analysis_id,
                'completed',
                summary=stats['summary'],
                mistake_ids=[m['$id'] for m in mistakes],
                completed_at=datetime.utcnow().isoformat() + 'Z'
            )
            
            logger.info(f"åˆ†æå®Œæˆ: {analysis_id}")
            return {
                'success': True,
                'analysis_id': analysis_id,
                'mistake_count': len(mistakes)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            await self._update_analysis_status(
                analysis_id,
                'failed',
                content=f'åˆ†æå¤±è´¥ï¼š{str(e)}'
            )
            raise
    
    async def _get_accumulated_mistakes(
        self,
        user_id: str,
        task_data: Dict[str, Any]
    ) -> List[Dict]:
        """
        è·å–ç”¨æˆ·ç§¯ç´¯çš„é”™é¢˜
        
        ç­–ç•¥ï¼šæŸ¥æ‰¾ accumulatedAnalyzedAt ä¸º null çš„é”™é¢˜
        è¿™äº›æ˜¯å°šæœªè¢«çº³å…¥ç§¯ç´¯åˆ†æçš„é”™é¢˜
        """
        from appwrite.query import Query
        
        logger.info(f"æŸ¥æ‰¾ç”¨æˆ· {user_id} æœªåˆ†æçš„ç§¯ç´¯é”™é¢˜ï¼ˆaccumulatedAnalyzedAt IS NULLï¼‰")
        
        mistakes = []
        offset = 0
        limit = 100
        
        while True:
            result = self.databases.list_documents(
                database_id=DATABASE_ID,
                collection_id=COLLECTION_MISTAKES,
                queries=[
                    Query.equal('userId', user_id),
                    Query.is_null('accumulatedAnalyzedAt'),  # æŸ¥æ‰¾æœªåˆ†æçš„é”™é¢˜
                    Query.limit(limit),
                    Query.offset(offset)
                ]
            )
            
            mistakes.extend(result['documents'])
            
            if len(result['documents']) < limit:
                break
            
            offset += limit
        
        logger.info(f"æ‰¾åˆ° {len(mistakes)} é“æœªåˆ†æçš„ç§¯ç´¯é”™é¢˜")
        return mistakes
    
    async def _calculate_statistics(
        self,
        mistakes: List[Dict],
        user_id: str
    ) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        from collections import Counter
        
        total_count = len(mistakes)
        
        # å­¦ç§‘åˆ†å¸ƒ
        subject_counts = Counter(m.get('subject', 'unknown') for m in mistakes)
        subject_distribution = [
            {
                'name': SUBJECT_NAMES.get(subject, subject),
                'count': count,
                'percentage': count / total_count * 100
            }
            for subject, count in subject_counts.most_common()
        ]
        
        # é”™å› åˆ†å¸ƒ
        reason_counts = Counter(m.get('errorReason', 'other') for m in mistakes)
        reason_distribution = [
            {
                'name': ERROR_REASON_NAMES.get(reason, reason),
                'count': count,
                'percentage': count / total_count * 100
            }
            for reason, count in reason_counts.most_common()
        ]
        
        return {
            'total_count': total_count,
            'subject_distribution': subject_distribution,
            'reason_distribution': reason_distribution,
            'summary': {
                'totalMistakes': total_count,
                'topSubject': subject_distribution[0]['name'] if subject_distribution else 'æ— ',
                'topReason': reason_distribution[0]['name'] if reason_distribution else 'æ— '
            }
        }
    
    async def _generate_analysis(
        self,
        analysis_id: str,
        mistakes: List[Dict],
        stats: Dict[str, Any]
    ) -> None:
        """
        ç”Ÿæˆåˆ†æå†…å®¹ï¼ˆæµå¼è¾“å‡ºï¼‰
        
        ä½¿ç”¨æµå¼ API å®æ—¶ç”Ÿæˆå†…å®¹ï¼Œå¹¶ä»¥ 0.5 ç§’é¢‘ç‡æ›´æ–°æ•°æ®åº“
        """
        # æ„å»º Prompt
        prompt = self._build_analysis_prompt(mistakes, stats)
        
        logger.info(f"å¼€å§‹ç”Ÿæˆåˆ†æå†…å®¹ï¼Œä½¿ç”¨æµå¼ LLM")
        
        try:
            # ä½¿ç”¨æµå¼è¾“å‡ºè°ƒç”¨ LLM
            stream_response = await self.llm_provider.chat(
                prompt=prompt,
                temperature=0.7,
                max_tokens=30000,  # å¢åŠ è¾“å‡ºé•¿åº¦é™åˆ¶ï¼Œå……åˆ†åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡
                stream=True  # å¯ç”¨æµå¼è¾“å‡º
            )
            
            # å¤„ç†æµå¼å“åº”
            await self._process_stream_response(analysis_id, stream_response)
            
        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _process_stream_response(
        self,
        analysis_id: str,
        stream_response: Any
    ) -> None:
        """
        å¤„ç†æµå¼å“åº”ï¼Œå®æ—¶æ›´æ–°æ•°æ®åº“
        
        ç­–ç•¥ï¼š
        1. å®æ—¶æ¥æ”¶ LLM æµå¼è¾“å‡º
        2. ç´¯ç§¯å†…å®¹å¹¶æŒ‰ 0.5 ç§’é¢‘ç‡æ›´æ–°æ•°æ®åº“
        3. é€šè¿‡ Appwrite Realtime è®©å‰ç«¯å®æ—¶çœ‹åˆ°å†…å®¹
        """
        accumulated_content = ''
        last_update_time = asyncio.get_event_loop().time()
        update_interval = 0.5  # 0.5 ç§’æ›´æ–°ä¸€æ¬¡
        
        logger.info("å¼€å§‹å¤„ç†æµå¼å“åº”")
        
        try:
            # éå†æµå¼å“åº”
            with stream_response:
                for chunk in stream_response:
                    # æå–å¢é‡å†…å®¹
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta.content is not None:
                            accumulated_content += delta.content
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®åº“
                            current_time = asyncio.get_event_loop().time()
                            if current_time - last_update_time >= update_interval:
                                # æ›´æ–°æ•°æ®åº“
                                await self._update_analysis_content(
                                    analysis_id,
                                    accumulated_content
                                )
                                last_update_time = current_time
                                logger.debug(f"æ›´æ–°åˆ†æå†…å®¹ï¼Œå½“å‰é•¿åº¦: {len(accumulated_content)}")
            
            # æœ€åä¸€æ¬¡æ›´æ–°ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½ä¿å­˜
            if accumulated_content:
                await self._update_analysis_content(analysis_id, accumulated_content)
                logger.info(f"æµå¼è¾“å‡ºå®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(accumulated_content)}")
        
        except Exception as e:
            logger.error(f"å¤„ç†æµå¼å“åº”å¤±è´¥: {e}", exc_info=True)
            raise
    
    async def _update_analysis_content(
        self,
        analysis_id: str,
        content: str
    ) -> None:
        """
        æ›´æ–°åˆ†æå†…å®¹åˆ°æ•°æ®åº“
        
        ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œå™¨é¿å…é˜»å¡
        """
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: self.databases.update_document(
                    database_id=DATABASE_ID,
                    collection_id=COLLECTION_ANALYSES,
                    document_id=analysis_id,
                    data={'analysisContent': content}
                )
            )
        except Exception as e:
            logger.warning(f"æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…ä¸­æ–­æµå¼è¾“å‡º
    
    def _build_analysis_prompt(
        self,
        mistakes: List[Dict],
        stats: Dict[str, Any]
    ) -> str:
        """æ„å»ºåˆ†æ Prompt"""
        
        total_count = stats['total_count']
        subject_dist = stats['subject_distribution']
        reason_dist = stats['reason_distribution']
        
        # æ ¼å¼åŒ–å­¦ç§‘åˆ†å¸ƒï¼ˆæ˜¾ç¤ºæ‰€æœ‰å­¦ç§‘ï¼‰
        subject_text = '\n'.join([
            f"  - {s['name']}: {s['count']}é“ ({s['percentage']:.1f}%)"
            for s in subject_dist
        ])
        
        # æ ¼å¼åŒ–é”™å› åˆ†å¸ƒï¼ˆæ˜¾ç¤ºæ‰€æœ‰é”™å› ï¼‰
        reason_text = '\n'.join([
            f"  - {r['name']}: {r['count']}é“ ({r['percentage']:.1f}%)"
            for r in reason_dist
        ])
        
        # æ ¼å¼åŒ–æ‰€æœ‰é”™é¢˜çš„è¯¦æƒ…åˆ—è¡¨
        mistakes_detail = self._format_mistakes_detail(mistakes)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€æ¸©æš–æœ‰çˆ±çš„å­¦ä¹ å¯¼å¸ˆï¼Œä¸ä»…æ“…é•¿åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æ¨¡å¼ï¼Œæ›´ç²¾é€šå„å­¦ç§‘çš„çŸ¥è¯†ç‚¹ã€å¸¸è§é¢˜å‹å’Œè§£é¢˜æŠ€å·§ã€‚

# å­¦ç”Ÿç§¯ç´¯é”™é¢˜æ¦‚å†µ

**é”™é¢˜æ€»æ•°**ï¼š{total_count} é“

## å­¦ç§‘åˆ†å¸ƒ
{subject_text}

## é”™å› åˆ†å¸ƒ
{reason_text}

# é”™é¢˜è¯¦ç»†ä¿¡æ¯

{mistakes_detail}

---

# ä½ çš„ä»»åŠ¡

è¯·åŸºäºä»¥ä¸Šå®Œæ•´çš„å­¦ä¹ æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½**æ·±åº¦å­¦ä¹ æŒ‡å¯¼æŠ¥å‘Š**ï¼ˆMarkdownæ ¼å¼ï¼‰ã€‚è¿™ä»½æŠ¥å‘Šè¦èƒ½çœŸæ­£å¸®åŠ©å­¦ç”Ÿçªç ´ç“¶é¢ˆã€æŒæ¡æ–¹æ³•ã€è·å¾—è¿›æ­¥ã€‚

## ğŸ“Š å­¦ä¹ ç°çŠ¶æ´å¯Ÿ

æ·±å…¥åˆ†æå­¦ç”Ÿçš„å­¦ä¹ çŠ¶å†µï¼Œç»“åˆå…·ä½“çš„é”™é¢˜å’Œé”™å› ï¼ŒæŒ‡å‡ºï¼š

### ä¸»è¦å­¦ä¹ ç›²åŒº
- å“ªäº›å­¦ç§‘/çŸ¥è¯†ç‚¹æ˜¯å½“å‰çš„è–„å¼±ç¯èŠ‚ï¼Ÿ
- è¿™äº›ç›²åŒºèƒŒåçš„æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
- ç»“åˆå…·ä½“é”™é¢˜è¯´æ˜é—®é¢˜æ‰€åœ¨

### çªå‡ºçš„é—®é¢˜æ¨¡å¼
- ä»é”™å› åˆ†å¸ƒçœ‹å‡ºä»€ä¹ˆè§„å¾‹ï¼Ÿ
- æ˜¯æ¦‚å¿µä¸æ¸…ã€æ€è·¯å—é˜»ï¼Œè¿˜æ˜¯ç²—å¿ƒå¤§æ„ï¼Ÿ
- ä¸åŒå­¦ç§‘æ˜¯å¦æœ‰å…±åŒçš„é—®é¢˜ï¼Ÿ

### å­¦ä¹ ä¼˜åŠ¿ä¸æ½œåŠ›
- æ­£å‘åé¦ˆï¼šç›®å‰åšå¾—å¥½çš„åœ°æ–¹
- å¯ä»¥å‘æŒ¥çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ
- å“ªäº›æ–¹é¢å·²ç»åœ¨è¿›æ­¥

## å­¦ä¹ çªç ´æŒ‡å—

### æ ¸å¿ƒæ”»åšç‚¹
æ˜ç¡®æŒ‡å‡º**å½“å‰æœ€åº”è¯¥æ”»å…‹çš„2-3ä¸ªæ ¸å¿ƒé—®é¢˜**ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›æ˜¯å…³é”®ï¼Œè§£å†³å®ƒä»¬èƒ½å¸¦æ¥ä»€ä¹ˆæ”¹å˜ã€‚

### å…·ä½“å­¦ä¹ æ–¹æ³•

é’ˆå¯¹é”™é¢˜ä¸­æš´éœ²çš„é—®é¢˜ï¼Œæä¾›**è¯¦ç»†çš„å­¦ä¹ æŒ‡å¯¼**ï¼š

**å¯¹äºæ¶‰åŠçš„é‡ç‚¹çŸ¥è¯†ç‚¹**ï¼Œæä¾›ï¼š
1. **æ¦‚å¿µæ¢³ç†**ï¼šè¿™ä¸ªçŸ¥è¯†ç‚¹çš„æ ¸å¿ƒæ˜¯ä»€ä¹ˆï¼Œå­¦ç”Ÿå®¹æ˜“æ··æ·†çš„åœ°æ–¹åœ¨å“ªé‡Œ
2. **è§£é¢˜æ€è·¯**ï¼šé‡åˆ°è¿™ç±»é¢˜ç›®åº”è¯¥æ€ä¹ˆæƒ³ã€æŒ‰ä»€ä¹ˆæ­¥éª¤æ¥
3. **æ˜“é”™æé†’**ï¼šå¸¸è§é™·é˜±å’Œæ³¨æ„äº‹é¡¹
4. **ç»ƒä¹ å»ºè®®**ï¼šå¯ä»¥åšä»€ä¹ˆç±»å‹çš„é¢˜æ¥å¼ºåŒ–

**å¯¹äºä¸»è¦é”™å› **ï¼ˆå¦‚æ¦‚å¿µä¸æ¸…ã€æ€è·¯æ–­è£‚ç­‰ï¼‰ï¼Œç»™å‡ºï¼š
1. **æ ¹æºåˆ†æ**ï¼šä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé—®é¢˜
2. **æ”¹è¿›æ–¹æ³•**ï¼šå…·ä½“æ€ä¹ˆåšæ‰èƒ½é¿å…
3. **å®æˆ˜æŠ€å·§**ï¼šè€ƒè¯•/åšé¢˜æ—¶çš„åº”å¯¹ç­–ç•¥

### å­¦ä¹ æ•ˆç‡æå‡

åŸºäºé”™é¢˜åæ˜ å‡ºçš„å­¦ä¹ ä¹ æƒ¯é—®é¢˜ï¼Œæä¾›ï¼š
- å¦‚ä½•æé«˜å­¦ä¹ æ•ˆç‡çš„æ–¹æ³•
- å¦‚ä½•å»ºç«‹çŸ¥è¯†ä½“ç³»
- å¦‚ä½•é¿å…é‡å¤çŠ¯é”™
- åˆ·é¢˜ä¸æ€»ç»“çš„å¹³è¡¡

## çŸ¥è¯†ç‚¹ç‚¹æ‹¨ä¸æŠ€å·§

é’ˆå¯¹é”™é¢˜ä¸­æ¶‰åŠçš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œæä¾›**å…·ä½“çš„ç‚¹æ‹¨å’ŒæŠ€å·§**ï¼š

### é‡ç‚¹çŸ¥è¯†ç‚¹è§£æ
é€‰æ‹©é”™é¢˜ä¸­æœ€å…³é”®çš„çŸ¥è¯†ç‚¹ï¼Œç»™å‡ºï¼š
- çŸ¥è¯†ç‚¹çš„æœ¬è´¨ç†è§£
- ä¸å…¶ä»–çŸ¥è¯†ç‚¹çš„è”ç³»
- è®°å¿†/ç†è§£çš„å°æŠ€å·§
- å…¸å‹é¢˜å‹çš„å¿«é€Ÿè¯†åˆ«æ–¹æ³•

### å­¦ç§‘é€šç”¨æŠ€å·§
æ ¹æ®å­¦ç§‘åˆ†å¸ƒï¼Œæä¾›ç›¸åº”å­¦ç§‘çš„ï¼š
- ç­”é¢˜æŠ€å·§
- æ£€éªŒæ–¹æ³•
- æ—¶é—´åˆ†é…ç­–ç•¥
- æåˆ†å…³é”®ç‚¹

## ğŸ’ª æˆé•¿å¯„è¯­

ç”¨**æ¸©æš–è€Œæœ‰åŠ›é‡çš„è¯**ï¼š
1. è‚¯å®šå­¦ç”Ÿè®°å½•é”™é¢˜ã€ä¸»åŠ¨å¤ç›˜çš„æ€åº¦
2. æŒ‡å‡ºé€šè¿‡è¿™æ¬¡åˆ†æçœ‹åˆ°çš„è¿›æ­¥ç©ºé—´
3. ç»™äºˆä¿¡å¿ƒå’Œæ–¹å‘ï¼šæ¯ä¸ªè–„å¼±ç‚¹éƒ½æ˜¯æˆé•¿ç‚¹ï¼Œæ¯æ¬¡çªç ´éƒ½è®©ä½ æ›´å¼ºå¤§

---

**æ’°å†™è¦æ±‚**ï¼š
- è¯­æ°”åƒä¸€ä½æ—¢ä¸“ä¸šåˆæ¸©æš–çš„å¯¼å¸ˆ
- åˆ†æè¦**åŸºäºå…·ä½“æ•°æ®å’Œé”™é¢˜**ï¼Œæœ‰ç†æœ‰æ®
- æŒ‡å¯¼è¦**è¯¦ç»†ã€å…·ä½“ã€å¯æ“ä½œ**ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
- çŸ¥è¯†ç‚¹ç‚¹æ‹¨è¦**å‡†ç¡®ã€å®ç”¨**ï¼Œèƒ½çœŸæ­£å¸®åŠ©ç†è§£
- é€‚åº¦ä½¿ç”¨ emoji å¢åŠ äº²å’ŒåŠ›
- ç¡®ä¿å­¦ç”Ÿçœ‹å®Œèƒ½æœ‰å®è´¨æ”¶è·

ç›´æ¥è¾“å‡º Markdown å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æˆ–å‰ç¼€ã€‚"""
        
        return prompt
    
    def _format_mistakes_detail(self, mistakes: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–é”™é¢˜è¯¦ç»†ä¿¡æ¯
        
        æ˜¾ç¤ºæ¯é“é¢˜çš„ï¼šå­¦ç§‘ã€é”™å› ã€å¤‡æ³¨ã€æ˜¯å¦é‡è¦
        ä¸é™åˆ¶æ•°é‡å’Œé•¿åº¦ï¼Œå……åˆ†åˆ©ç”¨ LLM çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›
        """
        if not mistakes:
            return "ï¼ˆæš‚æ— é”™é¢˜è¯¦æƒ…ï¼‰"
        
        total_count = len(mistakes)
        details = []
        
        for i, mistake in enumerate(mistakes, 1):
            subject = SUBJECT_NAMES.get(mistake.get('subject', ''), 'æœªçŸ¥å­¦ç§‘')
            error_reason = ERROR_REASON_NAMES.get(mistake.get('errorReason', ''), 'æœªæ ‡è®°')
            # ä½¿ç”¨ or '' æ¥å¤„ç† None å€¼
            note = (mistake.get('note') or '').strip()
            is_important = mistake.get('isImportant', False)
            
            # æ„å»ºå•æ¡é”™é¢˜ä¿¡æ¯
            detail = f"**é”™é¢˜ {i}** - {subject}"
            
            if is_important:
                detail += " ğŸ”´ é‡è¦"
            
            detail += f"\n- é”™å› ï¼š{error_reason}"
            
            if note:
                # ä¿ç•™å®Œæ•´å¤‡æ³¨å†…å®¹
                detail += f"\n- å¤‡æ³¨ï¼š{note}"
            
            details.append(detail)
        
        result = '\n\n'.join(details)
        result += f"\n\nï¼ˆä»¥ä¸Šä¸ºå…¨éƒ¨ {total_count} é“é”™é¢˜çš„è¯¦ç»†ä¿¡æ¯ï¼‰"
        
        return result
    
    async def _mark_mistakes_as_analyzed(
        self,
        mistakes: List[Dict]
    ) -> None:
        """
        æ ‡è®°æ‰€æœ‰é”™é¢˜ä¸ºå·²åˆ†æ
        
        æ›´æ–° accumulatedAnalyzedAt å­—æ®µä¸ºå½“å‰æ—¶é—´
        """
        if not mistakes:
            return
        
        current_time = datetime.utcnow().isoformat() + 'Z'
        logger.info(f"æ ‡è®° {len(mistakes)} é“é”™é¢˜ä¸ºå·²åˆ†æ")
        
        # æ‰¹é‡æ›´æ–°æ¯é“é”™é¢˜çš„ accumulatedAnalyzedAt å­—æ®µ
        for mistake in mistakes:
            mistake_id = mistake['$id']
            try:
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    lambda: self.databases.update_document(
                        database_id=DATABASE_ID,
                        collection_id=COLLECTION_MISTAKES,
                        document_id=mistake_id,
                        data={'accumulatedAnalyzedAt': current_time}
                    )
                )
                logger.debug(f"å·²æ ‡è®°é”™é¢˜ {mistake_id} ä¸ºå·²åˆ†æ")
            except Exception as e:
                logger.warning(f"æ›´æ–°é”™é¢˜ {mistake_id} å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–é”™é¢˜ï¼Œä¸ä¸­æ–­æµç¨‹
        
        logger.info(f"å®Œæˆæ ‡è®° {len(mistakes)} é“é”™é¢˜")
    
    async def _update_analysis_status(
        self,
        analysis_id: str,
        status: str,
        content: Optional[str] = None,
        summary: Optional[Dict] = None,
        mistake_ids: Optional[List[str]] = None,
        completed_at: Optional[str] = None
    ) -> None:
        """æ›´æ–°åˆ†æè®°å½•çŠ¶æ€"""
        import json
        
        data = {'status': status}
        
        if content is not None:
            data['analysisContent'] = content
        
        if summary is not None:
            data['summary'] = json.dumps(summary)  # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
        
        if mistake_ids is not None:
            data['mistakeIds'] = mistake_ids
        
        if completed_at is not None:
            data['completedAt'] = completed_at
        
        if status == 'processing':
            data['startedAt'] = datetime.utcnow().isoformat() + 'Z'
        
        try:
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                lambda: self.databases.update_document(
                    database_id=DATABASE_ID,
                    collection_id=COLLECTION_ANALYSES,
                    document_id=analysis_id,
                    data=data
                )
            )
            logger.info(f"æ›´æ–°åˆ†æçŠ¶æ€: {status}")
        except Exception as e:
            logger.error(f"æ›´æ–°åˆ†æçŠ¶æ€å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…ä¸­æ–­æµç¨‹

