"""
AIç§¯ç´¯é”™é¢˜åˆ†æ Function

ç”¨æˆ·è§¦å‘ç§¯ç´¯é”™é¢˜åˆ†ææ—¶è°ƒç”¨æ­¤ Function
Function è´Ÿè´£ï¼š
1. åˆ›å»ºåˆ†æè®°å½•ï¼ˆaccumulated_analysesï¼‰
2. è§¦å‘ Worker ä»»åŠ¡
3. è¿”å›åˆ†æè®°å½• ID ä¾›å‰ç«¯è®¢é˜…
"""
import os
import sys
import json
from datetime import datetime, timezone
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query

# æ·»åŠ  shared è·¯å¾„
sys.path.append('../shared')

try:
    from utils import success_response, error_response, parse_request_body
except ImportError:
    # æœ¬åœ°æµ‹è¯•æ—¶çš„é™çº§å¤„ç†
    def success_response(data, message="Success"):
        return {"success": True, "data": data, "message": message}
    
    def error_response(message, code=400, details=None):
        return {"success": False, "message": message, "code": code, "details": details}
    
    def parse_request_body(req):
        if hasattr(req, 'body'):
            body = req.body
            if isinstance(body, str):
                return json.loads(body) if body else {}
            return body
        return {}


# Configuration
DATABASE_ID = os.environ.get('APPWRITE_DATABASE_ID', 'main')
COLLECTION_ANALYSES = 'accumulated_analyses'
COLLECTION_MISTAKES = 'mistake_records'


def get_databases() -> Databases:
    """Initialize Databases service"""
    client = Client()
    client.set_endpoint(os.environ.get('APPWRITE_ENDPOINT', 'https://cloud.appwrite.io/v1'))
    client.set_project(os.environ['APPWRITE_PROJECT_ID'])
    client.set_key(os.environ['APPWRITE_API_KEY'])
    return Databases(client)


def trigger_worker_task(task_data: dict) -> bool:
    """
    è§¦å‘ Worker ä»»åŠ¡
    
    è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„ Worker è§¦å‘æœºåˆ¶å®ç°
    å¯ä»¥æ˜¯ï¼š
    1. å‘é€æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆæ¨èï¼‰
    2. ç›´æ¥ HTTP è°ƒç”¨ Worker API
    3. å†™å…¥ä»»åŠ¡è¡¨ï¼ŒWorker è½®è¯¢
    """
    try:
        import requests
        
        worker_url = os.environ.get('WORKER_API_URL', 'http://worker:8000')
        
        # æ„å»ºä»»åŠ¡è¯·æ±‚
        task_request = {
            'task_type': 'accumulated_mistakes_analyzer',
            'task_data': task_data,
            'priority': 3  # ä¸­ç­‰ä¼˜å…ˆçº§
        }
        
        print(f"[Worker Task] è§¦å‘ç§¯ç´¯é”™é¢˜åˆ†æ: {json.dumps(task_request)}")
        
        # è°ƒç”¨ Worker API
        response = requests.post(
            f'{worker_url}/tasks/enqueue',
            json=task_request,
            timeout=10
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"Worker ä»»åŠ¡å·²å…¥é˜Ÿ: {result.get('task_id')}")
            return True
        else:
            print(f"Worker å“åº”é”™è¯¯: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        print(f"è§¦å‘ Worker å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def calculate_days_since_last_review(databases: Databases, user_id: str) -> int:
    """è®¡ç®—è·ä¸Šæ¬¡å¤ç›˜çš„å¤©æ•°"""
    try:
        # æŸ¥æ‰¾æœ€è¿‘ä¸€æ¬¡å®Œæˆçš„åˆ†æ
        result = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ANALYSES,
            queries=[
                Query.equal('userId', user_id),
                Query.equal('status', 'completed'),
                Query.order_desc('$createdAt'),
                Query.limit(1)
            ]
        )
        
        if result['total'] > 0:
            last_review = result['documents'][0]['$createdAt']
            # è§£ææ—¥æœŸ
            last_date = datetime.fromisoformat(last_review.replace('Z', '+00:00'))
            now = datetime.utcnow()
            days = (now - last_date).days
            return days
        else:
            # é¦–æ¬¡åˆ†æï¼Œè¿”å›é»˜è®¤å€¼
            return 0
            
    except Exception as e:
        print(f"è®¡ç®—è·ä¸Šæ¬¡å¤ç›˜å¤©æ•°å¤±è´¥: {e}")
        return 0


def count_accumulated_mistakes(databases: Databases, user_id: str) -> int:
    """ç»Ÿè®¡ç§¯ç´¯çš„é”™é¢˜æ•°é‡ï¼ˆæœªè¢«åˆ†æçš„é”™é¢˜ï¼‰"""
    try:
        # ç»Ÿè®¡ accumulatedAnalyzedAt ä¸º null çš„é”™é¢˜æ•°é‡
        result = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_MISTAKES,
            queries=[
                Query.equal('userId', user_id),
                Query.is_null('accumulatedAnalyzedAt'),  # æŸ¥æ‰¾æœªåˆ†æçš„é”™é¢˜
                Query.limit(1)  # åªéœ€è¦ total æ•°é‡
            ]
        )
        
        return result['total']
        
    except Exception as e:
        print(f"ç»Ÿè®¡ç§¯ç´¯é”™é¢˜å¤±è´¥: {e}")
        return 0


def check_accumulated_analysis_limit(databases: Databases, user_id: str) -> tuple:
    """
    æ£€æŸ¥ç”¨æˆ·ç§¯ç´¯é”™é¢˜åˆ†æé™åˆ¶
    
    å…è´¹ç”¨æˆ·ï¼šæ¯å¤©æœ€å¤š 1 æ¬¡
    ä¼šå‘˜ç”¨æˆ·ï¼šæ— é™åˆ¶
    
    è¿”å›: (æ˜¯å¦å…è®¸, é”™è¯¯æ¶ˆæ¯, ç”¨æˆ·æ¡£æ¡ˆ)
    """
    try:
        # è·å–ç”¨æˆ·æ¡£æ¡ˆ
        profiles = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id='profiles',
            queries=[
                Query.equal('userId', user_id),
                Query.limit(1)
            ]
        )
        
        if not profiles['documents']:
            return False, "ç”¨æˆ·æ¡£æ¡ˆä¸å­˜åœ¨", None
        
        profile = profiles['documents'][0]
        
        # æ£€æŸ¥è®¢é˜…çŠ¶æ€
        subscription_status = profile.get('subscriptionStatus', 'free')
        
        # ä¼šå‘˜ç”¨æˆ·æ— é™åˆ¶
        if subscription_status == 'active':
            expiry_date = profile.get('subscriptionExpiryDate')
            if expiry_date:
                expiry_datetime = datetime.fromisoformat(expiry_date.replace('Z', '+00:00'))
                if expiry_datetime > datetime.now(timezone.utc):
                    return True, None, profile
        
        # å…è´¹ç”¨æˆ·ï¼šæ£€æŸ¥æ¯æ—¥é™åˆ¶
        today = datetime.now(timezone.utc).date()
        reset_date = profile.get('dailyLimitsResetDate')
        today_analyses = profile.get('todayAccumulatedAnalysis', 0)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®è®¡æ•°
        if reset_date:
            reset_datetime = datetime.fromisoformat(reset_date.replace('Z', '+00:00'))
            reset_date_only = reset_datetime.date()
            
            if reset_date_only < today:
                # éœ€è¦é‡ç½®
                today_analyses = 0
                databases.update_document(
                    database_id=DATABASE_ID,
                    collection_id='profiles',
                    document_id=profile['$id'],
                    data={
                        'todayAccumulatedAnalysis': 0,
                        'dailyLimitsResetDate': datetime.now(timezone.utc).isoformat()
                    }
                )
                profile['todayAccumulatedAnalysis'] = 0
        else:
            # é¦–æ¬¡ä½¿ç”¨
            databases.update_document(
                database_id=DATABASE_ID,
                collection_id='profiles',
                document_id=profile['$id'],
                data={
                    'dailyLimitsResetDate': datetime.now(timezone.utc).isoformat()
                }
            )
        
        # æ£€æŸ¥æ˜¯å¦è¶…é™ï¼ˆå…è´¹ç”¨æˆ·æ¯å¤©æœ€å¤š 1 æ¬¡ï¼‰
        FREE_USER_DAILY_LIMIT = 1
        if today_analyses >= FREE_USER_DAILY_LIMIT:
            return False, f"ä»Šæ—¥å…è´¹é¢åº¦å·²ç”¨å®Œï¼ˆ{FREE_USER_DAILY_LIMIT}/{FREE_USER_DAILY_LIMIT}ï¼‰ï¼Œå‡çº§ä¼šå‘˜äº«æ— é™åˆ¶", profile
        
        return True, None, profile
        
    except Exception as e:
        return False, f"æƒé™æ£€æŸ¥å¤±è´¥: {str(e)}", None


def increment_accumulated_analysis_count(databases: Databases, profile_id: str):
    """å¢åŠ ä»Šæ—¥ç§¯ç´¯åˆ†æè®¡æ•°"""
    try:
        profile = databases.get_document(
            database_id=DATABASE_ID,
            collection_id='profiles',
            document_id=profile_id
        )
        
        current_count = profile.get('todayAccumulatedAnalysis', 0)
        databases.update_document(
            database_id=DATABASE_ID,
            collection_id='profiles',
            document_id=profile_id,
            data={
                'todayAccumulatedAnalysis': current_count + 1
            }
        )
        return True
    except Exception as e:
        print(f"æ›´æ–°æ¯æ—¥è®¡æ•°å¤±è´¥: {str(e)}")
        return False


def main(context):
    """Main entry point for Appwrite Function"""
    try:
        req = context.req
        res = context.res
        
        # è§£æè¯·æ±‚
        body = parse_request_body(req)
        user_id = body.get('userId')
        
        if not user_id:
            return res.json(error_response("userId is required"))
        
        print(f"æ”¶åˆ°åˆ†æè¯·æ±‚: userId={user_id}")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        databases = get_databases()
        
        # ğŸ”’ æƒé™æ£€æŸ¥ï¼šç§¯ç´¯é”™é¢˜åˆ†ææ¯æ—¥é™åˆ¶
        is_allowed, error_msg, profile = check_accumulated_analysis_limit(databases, user_id)
        if not is_allowed:
            return res.json(error_response(error_msg, 403))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿›è¡Œä¸­çš„åˆ†æ
        existing = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ANALYSES,
            queries=[
                Query.equal('userId', user_id),
                Query.equal('status', ['pending', 'processing']),
                Query.limit(1)
            ]
        )
        
        if existing['total'] > 0:
            # å·²æœ‰è¿›è¡Œä¸­çš„åˆ†æï¼Œè¿”å›ç°æœ‰è®°å½•
            analysis = existing['documents'][0]
            return res.json(success_response({
                'analysisId': analysis['$id'],
                'status': analysis['status'],
                'message': 'å·²æœ‰åˆ†ææ­£åœ¨è¿›è¡Œä¸­'
            }))
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        days_since_last_review = calculate_days_since_last_review(databases, user_id)
        accumulated_mistakes = count_accumulated_mistakes(databases, user_id)
        
        print(f"ç»Ÿè®¡ä¿¡æ¯: days={days_since_last_review}, mistakes={accumulated_mistakes}")
        
        # åˆ›å»ºåˆ†æè®°å½•
        analysis_data = {
            'userId': user_id,
            'status': 'pending',
            'mistakeCount': accumulated_mistakes,
            'daysSinceLastReview': days_since_last_review,
            'analysisContent': '',
            'summary': json.dumps({}),  # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            'mistakeIds': [],
        }
        
        analysis = databases.create_document(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ANALYSES,
            document_id=ID.unique(),
            data=analysis_data
        )
        
        analysis_id = analysis['$id']
        print(f"åˆ›å»ºåˆ†æè®°å½•: {analysis_id}")
        
        # è§¦å‘ Worker ä»»åŠ¡
        task_data = {
            'analysis_id': analysis_id,
            'user_id': user_id,
            'mistake_count': accumulated_mistakes,
            'days_since_last_review': days_since_last_review
        }
        
        worker_triggered = trigger_worker_task(task_data)
        
        if not worker_triggered:
            # Worker è§¦å‘å¤±è´¥ï¼Œæ›´æ–°çŠ¶æ€
            databases.update_document(
                database_id=DATABASE_ID,
                collection_id=COLLECTION_ANALYSES,
                document_id=analysis_id,
                data={'status': 'failed'}
            )
            return res.json(error_response("Failed to trigger worker task", 500))
        
        # æ›´æ–°æ¯æ—¥è®¡æ•°ï¼ˆä»…å…è´¹ç”¨æˆ·éœ€è¦ï¼‰
        if profile and profile.get('subscriptionStatus') != 'active':
            increment_accumulated_analysis_count(databases, profile['$id'])
        
        # è¿”å›åˆ†æè®°å½• ID
        return res.json(success_response({
            'analysisId': analysis_id,
            'status': 'pending',
            'mistakeCount': accumulated_mistakes,
            'daysSinceLastReview': days_since_last_review,
            'message': 'åˆ†æä»»åŠ¡å·²åˆ›å»ºï¼Œè¯·è®¢é˜…æ›´æ–°'
        }))
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"Error: {error_details}")
        return res.json(error_response(str(e), 500, error_details))

